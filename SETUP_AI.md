# üõ†Ô∏è Gu√≠a de Configuraci√≥n - Generaci√≥n de Ejercicios con IA

## Problemas Corregidos

### 1. **Configuraci√≥n de Ollama**
- ‚úÖ Corregida URL de Ollama para Docker y modo local
- ‚úÖ Agregada detecci√≥n autom√°tica de URLs disponibles
- ‚úÖ Verificaci√≥n de modelo descargado

### 2. **Configuraci√≥n de ChromaDB**
- ‚úÖ Variables de entorno configurables
- ‚úÖ Manejo de errores mejorado
- ‚úÖ Logs detallados del procesamiento de PDFs

### 3. **Variables de Entorno**
- ‚úÖ Archivo `.env` creado con configuraci√≥n correcta

---

## üöÄ Configuraci√≥n R√°pida

### Opci√≥n 1: Desarrollo Local

#### 1. Instalar Ollama
```bash
# Windows: Descargar desde https://ollama.ai
# Linux/Mac:
curl -fsSL https://ollama.ai/install.sh | sh
```

#### 2. Descargar el modelo
```bash
ollama pull llama3
```

#### 3. Iniciar servicios
```bash
# Terminal 1: Iniciar base de datos y ChromaDB
docker-compose up db chroma

# Terminal 2: Verificar Ollama
ollama serve  # Si no est√° corriendo autom√°ticamente

# Terminal 3: Verificar configuraci√≥n
python check_ai_services.py

# Terminal 4: Iniciar backend
python -m uvicorn src.infrastructure.http.main:app --reload --host 0.0.0.0 --port 8000
```

#### 4. Configurar `.env` para local
Tu archivo `.env` ya est√° configurado para modo local:
```env
OLLAMA_BASE_URL=http://localhost:11434
CHROMA_DB_HOST=localhost
CHROMA_DB_PORT=8001
```

### Opci√≥n 2: Docker Completo

#### 1. Actualizar `.env` para Docker
Si vas a ejecutar todo en Docker, actualiza `.env`:
```env
OLLAMA_BASE_URL=http://ollama:11434
CHROMA_DB_HOST=chroma
CHROMA_DB_PORT=8000
```

#### 2. Iniciar servicios
```bash
docker-compose up -d
```

#### 3. Descargar modelo dentro del contenedor
```bash
docker exec -it fase_final_ollama ollama pull llama3
```

#### 4. Verificar estado
```bash
docker-compose ps
docker logs fase_final_backend
```

---

## üìã Verificaci√≥n de Configuraci√≥n

### Script de Verificaci√≥n
```bash
python check_ai_services.py
```

Este script verifica:
- ‚úÖ Ollama est√° corriendo
- ‚úÖ Modelo `llama3` est√° descargado
- ‚úÖ ChromaDB est√° disponible

### Salida Esperada
```
üîç Verificando configuraci√≥n de IA para el proyecto...

‚úÖ Ollama est√° corriendo en http://localhost:11434

üì¶ Modelos disponibles: ['llama3', 'llama2']
‚úÖ Modelo 'llama3' est√° disponible

‚úÖ ChromaDB est√° corriendo en http://localhost:8001

============================================================
üìä RESUMEN:
============================================================
Ollama:       ‚úÖ OK
Modelo llama3: ‚úÖ OK
ChromaDB:     ‚úÖ OK

‚úÖ Todo configurado correctamente. El sistema est√° listo.
```

---

## üß™ Probar la Generaci√≥n

### 1. Desde la UI (Frontend)
1. Ir a `/teacher/modules/{module_id}/create-activity`
2. Subir un PDF
3. Configurar par√°metros (topic, dificultad, lenguaje)
4. Click en "Generar Ejercicios con IA"

### 2. Desde API directamente
```bash
# 1. Crear actividad
curl -X POST http://localhost:8000/api/v1/teacher/activities \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Test Activity",
    "course_id": "default_course",
    "teacher_id": "teacher123",
    "instructions": "Test"
  }'

# Respuesta: {"id": "activity_id_123", ...}

# 2. Subir PDF
curl -X POST http://localhost:8000/api/v1/learning/activities/activity_id_123/document \
  -F "file=@tu_archivo.pdf"

# 3. Generar ejercicios
curl -X POST http://localhost:8000/api/v1/learning/generate \
  -H "Content-Type: application/json" \
  -d '{
    "activity_id": "activity_id_123",
    "topic": "Variables en Python",
    "difficulty": "medium",
    "language": "python",
    "count": 3
  }'
```

---

## üêõ Troubleshooting

### Error: "Could not connect to Ollama"

**Causa**: Ollama no est√° corriendo o URL incorrecta

**Soluci√≥n**:
```bash
# Verificar si Ollama est√° corriendo
curl http://localhost:11434/api/tags

# Si no responde, iniciar Ollama
ollama serve

# Verificar nuevamente
python check_ai_services.py
```

### Error: "Model 'llama3' not found"

**Causa**: Modelo no descargado

**Soluci√≥n**:
```bash
# Descargar modelo
ollama pull llama3

# Verificar modelos descargados
ollama list
```

### Error: "Failed to connect to ChromaDB"

**Causa**: ChromaDB no est√° corriendo

**Soluci√≥n**:
```bash
# Iniciar ChromaDB
docker-compose up chroma -d

# Verificar que est√° corriendo
curl http://localhost:8001/api/v1/heartbeat
```

### Error: "PDF file appears to be empty"

**Causa**: PDF corrupto o protegido

**Soluci√≥n**:
- Aseg√∫rate de que el PDF no est√© encriptado
- Verifica que el PDF contenga texto extra√≠ble (no solo im√°genes)
- Prueba con otro PDF

### No se generan ejercicios (sin error expl√≠cito)

**Verificar logs**:
```bash
# Docker
docker logs fase_final_backend -f

# Local
# Los logs aparecen en la terminal donde corriste uvicorn
```

**Buscar**:
- `[OllamaExerciseGenerator]` - Estado de generaci√≥n
- `[RagService]` - Procesamiento de PDFs
- `ERROR` o `EXCEPTION`

---

## üìä Monitoreo en Producci√≥n

### Variables de Entorno para Producci√≥n

Aseg√∫rate de configurar en tu ambiente de producci√≥n:

```env
# Producci√≥n (Cloud)
OLLAMA_BASE_URL=http://ollama:11434  # o la IP/dominio de tu servidor Ollama
CHROMA_DB_HOST=chroma  # o la IP/dominio de tu servidor ChromaDB
CHROMA_DB_PORT=8000
DATABASE_URL=postgresql://user:pass@host:port/db
SECRET_KEY=tu-secret-key-seguro-y-largo
DB_ECHO=False
```

### Health Check Endpoint

Agregar verificaci√≥n de servicios en el health check:

```python
# En main.py
@app.get("/health/ai")
def health_check_ai():
    """Verifica estado de servicios de IA"""
    ollama_ok = check_ollama()
    chroma_ok = check_chroma()
    return {
        "ollama": "ok" if ollama_ok else "error",
        "chromadb": "ok" if chroma_ok else "error"
    }
```

---

## üìù Notas Adicionales

### Modelos Ollama Recomendados

- **llama3** (Recomendado): Balance entre calidad y velocidad
- **llama2**: Alternativa m√°s ligera
- **codellama**: Especializado en c√≥digo (opcional)

### Rendimiento

- Generaci√≥n de 1 ejercicio: ~30-60 segundos
- Procesamiento de PDF (20 p√°ginas): ~10-15 segundos
- Genera ejercicios de 1 en 1 para m√°xima estabilidad

### L√≠mites

- Tama√±o m√°ximo de PDF: Ilimitado (pero considera tiempo de procesamiento)
- Ejercicios por generaci√≥n: 1-10 recomendado
- Timeout de generaci√≥n: 300 segundos (5 minutos)

---

## ‚úÖ Checklist de Configuraci√≥n

- [ ] Ollama instalado y corriendo
- [ ] Modelo `llama3` descargado
- [ ] ChromaDB corriendo
- [ ] PostgreSQL corriendo
- [ ] `.env` configurado correctamente
- [ ] `python check_ai_services.py` pasa todas las verificaciones
- [ ] Backend inicia sin errores
- [ ] Frontend conecta con backend

Si todos los items est√°n marcados, ¬°est√°s listo para generar ejercicios con IA! üéâ
